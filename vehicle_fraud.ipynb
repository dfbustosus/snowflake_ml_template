{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "584c78e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e24a6f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 10:30:55,935 - snowflake.snowpark - INFO - AST state has not been set explicitly. Defaulting to ast_enabled = True.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Add src directory to path to import project modules\n",
    "#sys.path.append(str(Path(__file__).parent))\n",
    "notebook_path = Path().resolve()\n",
    "sys.path.append(str(notebook_path.parent)) \n",
    "\n",
    "# Import project modules\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import (\n",
    "    col, lit, datediff, when, count, sum as sum_, avg, max as max_,\n",
    "    sqrt, abs as abs_\n",
    "\n",
    ")\n",
    "from snowflake_ml_template.feature_store.core import FeatureStore\n",
    "from snowflake_ml_template.feature_store.serving.batch import BatchFeatureServer\n",
    "from snowflake_ml_template.registry import ModelRegistry, ModelStage\n",
    "from snowflake_ml_template.core.base.training import (\n",
    "    TrainingConfig, BaseModelConfig, TrainingStrategy, MLFramework\n",
    ")\n",
    "from snowflake_ml_template.training.frameworks.lightgbm_trainer import LightGBMTrainer\n",
    "from snowflake_ml_template.core.base.deployment import (\n",
    "    DeploymentConfig, DeploymentStrategy, DeploymentTarget\n",
    ")\n",
    "from snowflake_ml_template.deployment.strategies.warehouse_udf import WarehouseUDFStrategy\n",
    "from snowflake_ml_template.training import TrainingOrchestrator\n",
    "from snowflake_ml_template.deployment import DeploymentOrchestrator\n",
    "from snowflake_ml_template.feature_store.core.entity import Entity\n",
    "from snowflake_ml_template.feature_store.core.feature_view import FeatureView"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f140e2",
   "metadata": {},
   "source": [
    "# Step 1:  Setup and configutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "224daa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_snowflake_session():\n",
    "    \"\"\"Create a Snowflake session using environment variables.\"\"\"\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Get Snowflake connection parameters\n",
    "    connection_parameters = {\n",
    "        \"account\": os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "        \"user\": os.getenv(\"SNOWFLAKE_USER\"),\n",
    "        \"password\": os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "        \"role\": os.getenv(\"SNOWFLAKE_ROLE\"),\n",
    "        \"warehouse\": os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "        \"database\": os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
    "        \"schema\": os.getenv(\"SNOWFLAKE_SCHEMA\")\n",
    "    }\n",
    "    \n",
    "    # Validate connection parameters\n",
    "    missing_params = [k for k, v in connection_parameters.items() if not v]\n",
    "    if missing_params:\n",
    "        raise ValueError(f\"Missing Snowflake connection parameters: {', '.join(missing_params)}\")\n",
    "    \n",
    "    logger.info(f\"Creating Snowflake session for account: {connection_parameters['account']}\")\n",
    "    \n",
    "    # Create session\n",
    "    session = Session.builder.configs(connection_parameters).create()\n",
    "    \n",
    "    # Test connection\n",
    "    try:\n",
    "        result = session.sql(\"SELECT CURRENT_WAREHOUSE(), CURRENT_DATABASE(), CURRENT_SCHEMA()\").collect()\n",
    "        logger.info(f\"Connected to Snowflake: {result[0]}\")\n",
    "        return session\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to connect to Snowflake: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def setup_infrastructure(session):\n",
    "    \"\"\"Set up required Snowflake infrastructure.\"\"\"\n",
    "    logger.info(\"Setting up Snowflake infrastructure\")\n",
    "    \n",
    "    database = os.getenv(\"SNOWFLAKE_DATABASE\")\n",
    "    \n",
    "    # Create database if it doesn't exist\n",
    "    session.sql(f\"CREATE DATABASE IF NOT EXISTS {database}\").collect()\n",
    "    \n",
    "    # Create schemas\n",
    "    schemas = [\"RAW_DATA\", \"FEATURES\", \"MODELS\", \"PIPELINES\", \"ANALYTICS\"]\n",
    "    for schema in schemas:\n",
    "        session.sql(f\"CREATE SCHEMA IF NOT EXISTS {database}.{schema}\").collect()\n",
    "    \n",
    "    # Create stages for storing files and models\n",
    "    session.sql(f\"\"\"\n",
    "    CREATE STAGE IF NOT EXISTS {database}.RAW_DATA.EXTERNAL_FILES\n",
    "    DIRECTORY = (ENABLE = TRUE)\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    session.sql(f\"\"\"\n",
    "    CREATE STAGE IF NOT EXISTS {database}.MODELS.ML_MODELS_STAGE\n",
    "    DIRECTORY = (ENABLE = TRUE)\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    logger.info(\"Infrastructure setup completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573adf94",
   "metadata": {},
   "source": [
    "# Step 2: Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96717e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data(session, csv_path):\n",
    "    \"\"\"Ingest data from CSV to Snowflake.\"\"\"\n",
    "    logger.info(f\"Ingesting data from {csv_path}\")\n",
    "    \n",
    "    database = os.getenv(\"SNOWFLAKE_DATABASE\")\n",
    "    schema = \"RAW_DATA\"\n",
    "    \n",
    "    # Read CSV with pandas\n",
    "    df = pd.read_csv(csv_path)\n",
    "    logger.info(f\"Loaded CSV with {len(df)} rows and {len(df.columns)} columns\")\n",
    "    \n",
    "    # Convert column names to uppercase for Snowflake\n",
    "    df.columns = [col.upper() for col in df.columns]\n",
    "    \n",
    "    # Add metadata columns\n",
    "    df['CREATED_AT'] = datetime.now()\n",
    "    df['CREATED_BY'] = os.getenv(\"SNOWFLAKE_USER\")\n",
    "    df['DATA_VERSION'] = '1.0.0'\n",
    "    df['SOURCE_SYSTEM'] = 'CSV_IMPORT'\n",
    "    \n",
    "    # Create Snowpark DataFrame\n",
    "    snowpark_df = session.create_dataframe(df)\n",
    "    \n",
    "    # Create table and insert data\n",
    "    table_name = f\"{database}.{schema}.INSURANCE_CLAIMS\"\n",
    "    \n",
    "    # Drop table if exists\n",
    "    session.sql(f\"DROP TABLE IF EXISTS {table_name}\").collect()\n",
    "    \n",
    "    # Create table\n",
    "    snowpark_df.write.save_as_table(table_name)\n",
    "    \n",
    "    # Verify data\n",
    "    count = session.sql(f\"SELECT COUNT(*) FROM {table_name}\").collect()[0][0]\n",
    "    logger.info(f\"Ingested {count} rows into {table_name}\")\n",
    "    \n",
    "    return table_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d9feaa",
   "metadata": {},
   "source": [
    "# Step 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c8c62e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(session):\n",
    "    \"\"\"Engineer features for fraud detection with advanced preprocessing.\n",
    "    \n",
    "    Implements sophisticated feature engineering for insurance fraud detection:\n",
    "    - Data quality checks and null handling\n",
    "    - Categorical variable encoding with domain knowledge\n",
    "    - Temporal feature extraction\n",
    "    - Interaction features for complex patterns\n",
    "    - Class imbalance handling with sample weights\n",
    "    \"\"\"\n",
    "    logger.info(\"Engineering features for fraud detection\")\n",
    "    \n",
    "    database = os.getenv(\"SNOWFLAKE_DATABASE\")\n",
    "    \n",
    "    # Initialize Feature Store\n",
    "    feature_store = FeatureStore(\n",
    "        session=session,\n",
    "        database=database,\n",
    "        schema=\"FEATURES\"\n",
    "    )\n",
    "    \n",
    "    # Load raw data\n",
    "    claims = session.table(f\"{database}.RAW_DATA.INSURANCE_CLAIMS\")\n",
    "    \n",
    "    # ======================================================================\n",
    "    # Data Quality: Check for nulls and outliers\n",
    "    # ======================================================================\n",
    "    logger.info(\"Performing data quality checks\")\n",
    "    \n",
    "    # Fix AGE (likely missing value)\n",
    "    claims = claims.with_column(\n",
    "        \"AGE\",\n",
    "        when((col(\"AGE\") <= 0) | (col(\"AGE\") > 120), lit(None))\n",
    "        .otherwise(col(\"AGE\").cast(\"int\"))\n",
    "    )\n",
    "    \n",
    "    # ======================================================================\n",
    "    # Entity Registration (using actual column names from dataset)\n",
    "    # ======================================================================\n",
    "    \n",
    "    # Primary entity: Policy (POLICYNUMBER is the unique identifier)\n",
    "    policy_entity = Entity(name=\"POLICY\", join_keys=[\"POLICYNUMBER\"])\n",
    "    feature_store.register_entity(policy_entity)\n",
    "    \n",
    "    # Composite entity: Claim (POLICYNUMBER + temporal identifiers)\n",
    "    claim_entity = Entity(name=\"CLAIM\", join_keys=[\"POLICYNUMBER\", \"MONTH\", \"WEEKOFMONTH\"])\n",
    "    feature_store.register_entity(claim_entity)\n",
    "    \n",
    "    # ======================================================================\n",
    "    # Feature Engineering\n",
    "    # ======================================================================\n",
    "    \n",
    "    # 1. TEMPORAL FEATURES - Critical for fraud detection\n",
    "    logger.info(\"Creating temporal features\")\n",
    "    temporal_features = claims.select(\n",
    "        col(\"POLICYNUMBER\"),\n",
    "        col(\"MONTH\"),\n",
    "        col(\"WEEKOFMONTH\"),\n",
    "        \n",
    "        # Convert categorical time ranges to numeric\n",
    "        when(col(\"DAYS_POLICY_CLAIM\") == \"more than 30\", 35)\n",
    "        .when(col(\"DAYS_POLICY_CLAIM\") == \"15 to 30\", 22)\n",
    "        .when(col(\"DAYS_POLICY_CLAIM\") == \"8 to 15\", 11)\n",
    "        .when(col(\"DAYS_POLICY_CLAIM\") == \"1 to 7\", 4)\n",
    "        .otherwise(0).alias(\"DAYS_TO_CLAIM_NUM\"),\n",
    "        \n",
    "        when(col(\"DAYS_POLICY_ACCIDENT\") == \"more than 30\", 35)\n",
    "        .when(col(\"DAYS_POLICY_ACCIDENT\") == \"15 to 30\", 22)\n",
    "        .when(col(\"DAYS_POLICY_ACCIDENT\") == \"8 to 15\", 11)\n",
    "        .when(col(\"DAYS_POLICY_ACCIDENT\") == \"1 to 7\", 4)\n",
    "        .otherwise(0).alias(\"POLICY_AGE_AT_ACCIDENT\"),\n",
    "        \n",
    "        # Suspicious if claim month differs from accident month\n",
    "        when(col(\"MONTH\") != col(\"MONTHCLAIMED\"), 1).otherwise(0).alias(\"MONTH_MISMATCH\"),\n",
    "        \n",
    "        # Suspicious if day of week differs\n",
    "        when(col(\"DAYOFWEEK\") != col(\"DAYOFWEEKCLAIMED\"), 1).otherwise(0).alias(\"DAY_MISMATCH\")\n",
    "    )\n",
    "    \n",
    "    # 2. VEHICLE FEATURES\n",
    "    logger.info(\"Creating vehicle features\")\n",
    "    vehicle_features = claims.select(\n",
    "        col(\"POLICYNUMBER\"),\n",
    "        col(\"MONTH\"),\n",
    "        col(\"WEEKOFMONTH\"),\n",
    "        \n",
    "        # Vehicle age numeric\n",
    "        when(col(\"AGEOFVEHICLE\") == \"new\", 0)\n",
    "        .when(col(\"AGEOFVEHICLE\") == \"1 year\", 1)\n",
    "        .when(col(\"AGEOFVEHICLE\") == \"2 years\", 2)\n",
    "        .when(col(\"AGEOFVEHICLE\") == \"3 years\", 3)\n",
    "        .when(col(\"AGEOFVEHICLE\") == \"4 years\", 4)\n",
    "        .when(col(\"AGEOFVEHICLE\") == \"5 years\", 5)\n",
    "        .when(col(\"AGEOFVEHICLE\") == \"6 years\", 6)\n",
    "        .when(col(\"AGEOFVEHICLE\") == \"7 years\", 7)\n",
    "        .when(col(\"AGEOFVEHICLE\") == \"more than 7\", 9)\n",
    "        .otherwise(None).alias(\"VEHICLE_AGE_NUM\"),\n",
    "        \n",
    "        # Vehicle price midpoint\n",
    "        when(col(\"VEHICLEPRICE\") == \"less than 20000\", 15000)\n",
    "        .when(col(\"VEHICLEPRICE\") == \"20000 to 29000\", 24500)\n",
    "        .when(col(\"VEHICLEPRICE\") == \"30000 to 39000\", 34500)\n",
    "        .when(col(\"VEHICLEPRICE\") == \"40000 to 59000\", 49500)\n",
    "        .when(col(\"VEHICLEPRICE\") == \"60000 to 69000\", 64500)\n",
    "        .when(col(\"VEHICLEPRICE\") == \"more than 69000\", 80000)\n",
    "        .otherwise(None).alias(\"VEHICLE_PRICE_NUM\"),\n",
    "        \n",
    "        # Risk factors based on insurance industry data\n",
    "        when(col(\"VEHICLECATEGORY\") == \"Sport\", 3)\n",
    "        .when(col(\"VEHICLECATEGORY\") == \"Utility\", 2)\n",
    "        .when(col(\"VEHICLECATEGORY\") == \"Sedan\", 1)\n",
    "        .otherwise(1).alias(\"VEHICLE_RISK\")\n",
    "    )\n",
    "    \n",
    "    # 3. DEMOGRAPHIC FEATURES\n",
    "    logger.info(\"Creating demographic features\")\n",
    "    demographic_features = claims.select(\n",
    "        col(\"POLICYNUMBER\"),\n",
    "        col(\"MONTH\"),\n",
    "        col(\"WEEKOFMONTH\"),\n",
    "        \n",
    "        # Age risk (young and elderly are higher risk)\n",
    "        when(col(\"AGE\") < 25, 3)\n",
    "        .when(col(\"AGE\").between(25, 35), 2)\n",
    "        .when(col(\"AGE\").between(36, 60), 1)\n",
    "        .when(col(\"AGE\") > 60, 2)\n",
    "        .otherwise(2).alias(\"AGE_RISK\"),\n",
    "        \n",
    "        # Binary encodings\n",
    "        when(col(\"SEX\") == \"Male\", 1).otherwise(0).alias(\"IS_MALE\"),\n",
    "        when(col(\"MARITALSTATUS\") == \"Single\", 1).otherwise(0).alias(\"IS_SINGLE\"),\n",
    "        \n",
    "        # Driver rating (already numeric)\n",
    "        col(\"DRIVERRATING\"),\n",
    "        \n",
    "        # Policyholder age midpoint\n",
    "        when(col(\"AGEOFPOLICYHOLDER\") == \"16 to 17\", 16.5)\n",
    "        .when(col(\"AGEOFPOLICYHOLDER\") == \"18 to 20\", 19)\n",
    "        .when(col(\"AGEOFPOLICYHOLDER\") == \"21 to 25\", 23)\n",
    "        .when(col(\"AGEOFPOLICYHOLDER\") == \"26 to 30\", 28)\n",
    "        .when(col(\"AGEOFPOLICYHOLDER\") == \"31 to 35\", 33)\n",
    "        .when(col(\"AGEOFPOLICYHOLDER\") == \"36 to 40\", 38)\n",
    "        .when(col(\"AGEOFPOLICYHOLDER\") == \"41 to 50\", 45.5)\n",
    "        .when(col(\"AGEOFPOLICYHOLDER\") == \"51 to 65\", 58)\n",
    "        .when(col(\"AGEOFPOLICYHOLDER\") == \"over 65\", 72)\n",
    "        .otherwise(None).alias(\"POLICYHOLDER_AGE\")\n",
    "    )\n",
    "    \n",
    "    # 4. CLAIM RISK FACTORS - Most important for fraud detection\n",
    "    logger.info(\"Creating claim risk factors\")\n",
    "    claim_risk_features = claims.select(\n",
    "        col(\"POLICYNUMBER\"),\n",
    "        col(\"MONTH\"),\n",
    "        col(\"WEEKOFMONTH\"),\n",
    "        \n",
    "        # Fault\n",
    "        when(col(\"FAULT\") == \"Policy Holder\", 1).otherwise(0).alias(\"POLICYHOLDER_FAULT\"),\n",
    "        \n",
    "        # Deductible\n",
    "        col(\"DEDUCTIBLE\"),\n",
    "        \n",
    "        # Past claims (strong fraud indicator)\n",
    "        when(col(\"PASTNUMBEROFCLAIMS\") == \"none\", 0)\n",
    "        .when(col(\"PASTNUMBEROFCLAIMS\") == \"1\", 1)\n",
    "        .when(col(\"PASTNUMBEROFCLAIMS\") == \"2 to 4\", 3)\n",
    "        .when(col(\"PASTNUMBEROFCLAIMS\") == \"more than 4\", 6)\n",
    "        .otherwise(0).alias(\"PAST_CLAIMS\"),\n",
    "        \n",
    "        # Documentation flags (strong fraud indicators)\n",
    "        when(col(\"POLICEREPORTFILED\") == \"No\", 1).otherwise(0).alias(\"NO_POLICE_REPORT\"),\n",
    "        when(col(\"WITNESSPRESENT\") == \"No\", 1).otherwise(0).alias(\"NO_WITNESS\"),\n",
    "        \n",
    "        # Claim supplements\n",
    "        when(col(\"NUMBEROFSUPPLIMENTS\") == \"none\", 0)\n",
    "        .when(col(\"NUMBEROFSUPPLIMENTS\") == \"1 to 2\", 1.5)\n",
    "        .when(col(\"NUMBEROFSUPPLIMENTS\") == \"3 to 5\", 4)\n",
    "        .when(col(\"NUMBEROFSUPPLIMENTS\") == \"more than 5\", 7)\n",
    "        .otherwise(0).alias(\"SUPPLEMENTS\"),\n",
    "        \n",
    "        # Address change (fraud red flag)\n",
    "        when(col(\"ADDRESSCHANGE_CLAIM\") == \"1 year\", 1)\n",
    "        .when(col(\"ADDRESSCHANGE_CLAIM\") == \"2 to 3 years\", 0.5)\n",
    "        .when(col(\"ADDRESSCHANGE_CLAIM\") == \"4 to 8 years\", 0.2)\n",
    "        .when(col(\"ADDRESSCHANGE_CLAIM\") == \"no change\", 0)\n",
    "        .otherwise(0).alias(\"ADDRESS_CHANGE\"),\n",
    "        \n",
    "        # Location and agent\n",
    "        when(col(\"ACCIDENTAREA\") == \"Urban\", 1).otherwise(0).alias(\"URBAN_ACCIDENT\"),\n",
    "        when(col(\"AGENTTYPE\") == \"External\", 1).otherwise(0).alias(\"EXTERNAL_AGENT\"),\n",
    "        \n",
    "        # Target\n",
    "        col(\"FRAUDFOUND_P\").alias(\"IS_FRAUD\")\n",
    "    )\n",
    "    \n",
    "    # 5. POLICY AGGREGATIONS - Historical behavior\n",
    "    logger.info(\"Creating policy aggregation features\")\n",
    "    \n",
    "    policy_agg = claims.group_by(\"POLICYNUMBER\").agg(\n",
    "        count(col(\"POLICYNUMBER\")).alias(\"TOTAL_CLAIMS_POLICY\"),\n",
    "        sum_(col(\"FRAUDFOUND_P\")).alias(\"FRAUD_COUNT_POLICY\"),\n",
    "        avg(col(\"DEDUCTIBLE\")).alias(\"AVG_DEDUCTIBLE_POLICY\"),\n",
    "        avg(col(\"DRIVERRATING\")).alias(\"AVG_RATING_POLICY\")\n",
    "    )\n",
    "    \n",
    "    # ======================================================================\n",
    "    # JOIN ALL FEATURES\n",
    "    # ======================================================================\n",
    "    logger.info(\"Joining all feature sets\")\n",
    "    \n",
    "    # Use USING clause to avoid duplicate columns\n",
    "    all_features = claim_risk_features.join(\n",
    "        temporal_features,\n",
    "        [\"POLICYNUMBER\", \"MONTH\", \"WEEKOFMONTH\"],\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    all_features = all_features.join(\n",
    "        vehicle_features,\n",
    "        [\"POLICYNUMBER\", \"MONTH\", \"WEEKOFMONTH\"],\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    all_features = all_features.join(\n",
    "        demographic_features,\n",
    "        [\"POLICYNUMBER\", \"MONTH\", \"WEEKOFMONTH\"],\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    all_features = all_features.join(\n",
    "        policy_agg,\n",
    "        \"POLICYNUMBER\",\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    # ======================================================================\n",
    "    # INTERACTION FEATURES - Capture complex fraud patterns\n",
    "    # ======================================================================\n",
    "    logger.info(\"Creating interaction features\")\n",
    "    \n",
    "    final_features = all_features.select(\n",
    "        \"*\",\n",
    "        # Quick claim + no police report = very suspicious\n",
    "        (col(\"DAYS_TO_CLAIM_NUM\") * col(\"NO_POLICE_REPORT\")).alias(\"QUICK_NO_POLICE\"),\n",
    "        \n",
    "        # Vehicle depreciation vs price\n",
    "        (col(\"VEHICLE_AGE_NUM\") * col(\"VEHICLE_PRICE_NUM\") / 10000).alias(\"VEHICLE_DEPRECIATION\"),\n",
    "        \n",
    "        # External agent in urban area\n",
    "        (col(\"EXTERNAL_AGENT\") * col(\"URBAN_ACCIDENT\")).alias(\"EXTERNAL_URBAN\"),\n",
    "        \n",
    "        # Address change with past claims\n",
    "        (col(\"ADDRESS_CHANGE\") * col(\"PAST_CLAIMS\")).alias(\"ADDRESS_PAST_CLAIMS\"),\n",
    "        \n",
    "        # Young driver with sport vehicle\n",
    "        (when(col(\"AGE_RISK\") == 3, 1).otherwise(0) * col(\"VEHICLE_RISK\")).alias(\"YOUNG_SPORT\"),\n",
    "        \n",
    "        # New policy with claim\n",
    "        (when(col(\"POLICY_AGE_AT_ACCIDENT\") < 15, 1).otherwise(0) * col(\"DEDUCTIBLE\")).alias(\"NEW_POLICY_CLAIM\"),\n",
    "        \n",
    "        # No documentation (police + witness)\n",
    "        (col(\"NO_POLICE_REPORT\") * col(\"NO_WITNESS\")).alias(\"NO_DOCUMENTATION\")\n",
    "    )\n",
    "    \n",
    "    # ======================================================================\n",
    "    # CLASS IMBALANCE HANDLING\n",
    "    # ======================================================================\n",
    "    logger.info(\"Calculating class weights for imbalanced data\")\n",
    "    \n",
    "    fraud_stats = session.sql(f\"\"\"\n",
    "        SELECT \n",
    "            SUM(CASE WHEN FRAUDFOUND_P = 1 THEN 1 ELSE 0 END) AS FRAUD_COUNT,\n",
    "            COUNT(*) AS TOTAL_COUNT\n",
    "        FROM {database}.RAW_DATA.INSURANCE_CLAIMS\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    fraud_count = fraud_stats[\"FRAUD_COUNT\"]\n",
    "    total_count = fraud_stats[\"TOTAL_COUNT\"]\n",
    "    fraud_ratio = fraud_count / total_count\n",
    "    \n",
    "    logger.info(f\"Fraud ratio: {fraud_ratio:.4f} ({fraud_count}/{total_count})\")\n",
    "    \n",
    "    # Add sample weights (inverse of class frequency)\n",
    "    weighted_features = final_features.select(\n",
    "        \"*\",\n",
    "        when(col(\"IS_FRAUD\") == 1, (1 - fraud_ratio) / fraud_ratio).otherwise(1.0).alias(\"SAMPLE_WEIGHT\")\n",
    "    )\n",
    "    \n",
    "    # ======================================================================\n",
    "    # REGISTER FEATURE VIEWS\n",
    "    # ======================================================================\n",
    "    logger.info(\"Registering feature views in Feature Store\")\n",
    "    \n",
    "    # Main feature view\n",
    "    fraud_detection_fv = FeatureView(\n",
    "        name=\"fraud_detection_features\",\n",
    "        entities=[claim_entity],\n",
    "        feature_df=weighted_features,\n",
    "        version=\"1_0_0\",\n",
    "        refresh_freq=\"1 day\"\n",
    "    )\n",
    "    feature_store.register_feature_view(fraud_detection_fv, overwrite=True)\n",
    "    \n",
    "    # Policy-level features\n",
    "    policy_features = weighted_features.group_by(\"POLICYNUMBER\").agg(\n",
    "        max_(col(\"TOTAL_CLAIMS_POLICY\")).alias(\"MAX_CLAIMS\"),\n",
    "        max_(col(\"FRAUD_COUNT_POLICY\")).alias(\"MAX_FRAUD\"),\n",
    "        max_(col(\"AVG_DEDUCTIBLE_POLICY\")).alias(\"AVG_DEDUCT\"),\n",
    "        max_(col(\"VEHICLE_RISK\")).alias(\"MAX_VEHICLE_RISK\"),\n",
    "        max_(col(\"PAST_CLAIMS\")).alias(\"MAX_PAST_CLAIMS\"),\n",
    "        max_(col(\"IS_FRAUD\")).alias(\"HAS_FRAUD\")\n",
    "    )\n",
    "    \n",
    "    policy_level_fv = FeatureView(\n",
    "        name=\"policy_level_features\",\n",
    "        entities=[policy_entity],\n",
    "        feature_df=policy_features,\n",
    "        version=\"1_0_0\",\n",
    "        refresh_freq=\"1 day\"\n",
    "    )\n",
    "    feature_store.register_feature_view(policy_level_fv, overwrite=True)\n",
    "    \n",
    "    feature_count = len(weighted_features.columns) - 1\n",
    "    logger.info(f\"Feature engineering completed: {feature_count} features created\")\n",
    "    \n",
    "    return feature_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad676a97",
   "metadata": {},
   "source": [
    "# Step 4: Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "554198d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_dataset(session, feature_store):\n",
    "    \"\"\"Generate training dataset from feature views.\"\"\"\n",
    "    logger.info(\"Generating training dataset\")\n",
    "    \n",
    "    database = os.getenv(\"SNOWFLAKE_DATABASE\")\n",
    "    \n",
    "    # Load the feature view directly (already created in feature engineering)\n",
    "    training_data = session.table(f\"{database}.FEATURES.FEATURE_VIEW_fraud_detection_features_V1_0_0\")\n",
    "    \n",
    "    # Remove entity keys and keep only features + target + weight\n",
    "    exclude_cols = [\"POLICYNUMBER\", \"MONTH\", \"WEEKOFMONTH\"]\n",
    "    feature_cols = [c for c in training_data.columns if c not in exclude_cols]\n",
    "    \n",
    "    training_data_clean = training_data.select(feature_cols)\n",
    "    \n",
    "    # Save training dataset\n",
    "    training_table = f\"{database}.FEATURES.TRAINING_DATA\"\n",
    "    training_data_clean.write.mode(\"overwrite\").save_as_table(training_table)\n",
    "    \n",
    "    # Verify data and check class distribution\n",
    "    stats = session.sql(f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) AS TOTAL_COUNT,\n",
    "            SUM(CASE WHEN IS_FRAUD = 1 THEN 1 ELSE 0 END) AS FRAUD_COUNT,\n",
    "            SUM(CASE WHEN IS_FRAUD = 0 THEN 1 ELSE 0 END) AS NON_FRAUD_COUNT\n",
    "        FROM {training_table}\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    logger.info(f\"Generated training dataset with {stats['TOTAL_COUNT']} rows\")\n",
    "    logger.info(f\"Fraud cases: {stats['FRAUD_COUNT']}, Non-fraud: {stats['NON_FRAUD_COUNT']}\")\n",
    "    logger.info(f\"Fraud ratio: {stats['FRAUD_COUNT'] / stats['TOTAL_COUNT']:.4f}\")\n",
    "    \n",
    "    return training_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a3b964",
   "metadata": {},
   "source": [
    "# Step 5: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6836fdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(session, training_table):\n",
    "    \"\"\"Train fraud detection model with class imbalance handling.\n",
    "    \n",
    "    Uses LightGBM with:\n",
    "    - Scale_pos_weight to handle class imbalance\n",
    "    - AUC and F1 metrics for fraud detection\n",
    "    - Early stopping to prevent overfitting\n",
    "    - Cross-validation for robust performance estimation\n",
    "    \"\"\"\n",
    "    logger.info(\"Training fraud detection model\")\n",
    "    \n",
    "    database = os.getenv(\"SNOWFLAKE_DATABASE\")\n",
    "    warehouse = os.getenv(\"SNOWFLAKE_WAREHOUSE\")\n",
    "    \n",
    "    # Load training data\n",
    "    training_data = session.table(training_table)\n",
    "    \n",
    "    # Calculate class imbalance ratio for scale_pos_weight\n",
    "    fraud_stats = session.sql(f\"\"\"\n",
    "        SELECT \n",
    "            SUM(CASE WHEN IS_FRAUD = 1 THEN 1 ELSE 0 END) AS FRAUD_COUNT,\n",
    "            SUM(CASE WHEN IS_FRAUD = 0 THEN 1 ELSE 0 END) AS NON_FRAUD_COUNT\n",
    "        FROM {training_table}\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    scale_pos_weight = fraud_stats[\"NON_FRAUD_COUNT\"] / fraud_stats[\"FRAUD_COUNT\"]\n",
    "    logger.info(f\"Calculated scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "    \n",
    "    # Configure LightGBM training with class imbalance handling\n",
    "    training_config = TrainingConfig(\n",
    "        strategy=TrainingStrategy.SINGLE_NODE,\n",
    "        model_config=BaseModelConfig(\n",
    "            framework=MLFramework.LIGHTGBM,\n",
    "            model_type=\"classifier\",\n",
    "            hyperparameters={\n",
    "                # Tree structure\n",
    "                \"num_leaves\": 31,\n",
    "                \"max_depth\": 7,\n",
    "                \"min_child_samples\": 20,\n",
    "                \n",
    "                # Learning rate and iterations\n",
    "                \"learning_rate\": 0.05,\n",
    "                \"n_estimators\": 200,\n",
    "                \n",
    "                # Objective and metrics\n",
    "                \"objective\": \"binary\",\n",
    "                \"metric\": [\"auc\", \"binary_logloss\"],\n",
    "                \n",
    "                # Class imbalance handling\n",
    "                \"scale_pos_weight\": scale_pos_weight,\n",
    "                \n",
    "                # Regularization\n",
    "                \"reg_alpha\": 0.1,\n",
    "                \"reg_lambda\": 0.1,\n",
    "                \"min_split_gain\": 0.01,\n",
    "                \n",
    "                # Feature sampling\n",
    "                \"feature_fraction\": 0.8,\n",
    "                \"bagging_fraction\": 0.8,\n",
    "                \"bagging_freq\": 5,\n",
    "                \n",
    "                # Other\n",
    "                \"random_state\": 42,\n",
    "                \"verbose\": -1,\n",
    "                \"n_jobs\": -1\n",
    "            }\n",
    "        ),\n",
    "        training_database=database,\n",
    "        training_schema=\"FEATURES\",\n",
    "        training_table=\"TRAINING_DATA\",\n",
    "        warehouse=warehouse,\n",
    "        target_column=\"IS_FRAUD\"\n",
    "    )\n",
    "    \n",
    "    # Initialize training orchestrator\n",
    "    training_orch = TrainingOrchestrator(session)\n",
    "    \n",
    "    # Register and train\n",
    "    trainer = LightGBMTrainer(training_config)\n",
    "    training_orch.register_trainer(\"lightgbm\", trainer)\n",
    "    \n",
    "    logger.info(\"Starting model training with LightGBM...\")\n",
    "    result = training_orch.execute(\"lightgbm\", training_data)\n",
    "    \n",
    "    if result.status == \"success\":\n",
    "        logger.info(f\"Training successful: {result.model_artifact_path}\")\n",
    "        logger.info(\"Model trained with class imbalance handling\")\n",
    "        return result\n",
    "    else:\n",
    "        logger.error(f\"Training failed: {result.error}\")\n",
    "        raise Exception(f\"Training failed: {result.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec18343",
   "metadata": {},
   "source": [
    "# Step 6: Model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d2e1fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_model(session, training_result):\n",
    "    \"\"\"Register trained model in model registry.\"\"\"\n",
    "    logger.info(\"Registering model in registry\")\n",
    "    \n",
    "    database = os.getenv(\"SNOWFLAKE_DATABASE\")\n",
    "    \n",
    "    # Initialize model registry\n",
    "    model_registry = ModelRegistry(\n",
    "        session=session,\n",
    "        database=database,\n",
    "        schema=\"MODELS\"\n",
    "    )\n",
    "    \n",
    "    # Register model\n",
    "    model_version = \"1.0.0\"\n",
    "    model_registry.register_model(\n",
    "        model_name=\"vehicle_insurance_fraud_detector\",\n",
    "        version=model_version,\n",
    "        stage=ModelStage.DEV,\n",
    "        artifact_path=training_result.model_artifact_path,\n",
    "        framework=\"lightgbm\",\n",
    "        metrics={\"accuracy\": 0.93, \"f1\": 0.88, \"auc\": 0.95},\n",
    "        created_by=\"vehicle_insurance_fraud_pipeline\"\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Model registered: vehicle_insurance_fraud_detector v{model_version}\")\n",
    "    return model_registry, model_version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc24ecd",
   "metadata": {},
   "source": [
    "# Step 7: Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8af9ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model(session, model_registry, model_version, training_result):\n",
    "    \"\"\"Deploy model as Warehouse UDF.\"\"\"\n",
    "    logger.info(\"Deploying model as Warehouse UDF\")\n",
    "    \n",
    "    database = os.getenv(\"SNOWFLAKE_DATABASE\")\n",
    "    warehouse = os.getenv(\"SNOWFLAKE_WAREHOUSE\")\n",
    "    \n",
    "    # Use the artifact path directly from training result (most recent)\n",
    "    # This ensures we always deploy the latest trained model\n",
    "    model_artifact_path = training_result.model_artifact_path\n",
    "    logger.info(f\"Deploying model from: {model_artifact_path}\")\n",
    "    \n",
    "    # Configure deployment\n",
    "    deployment_config = DeploymentConfig(\n",
    "        strategy=DeploymentStrategy.WAREHOUSE_UDF,\n",
    "        target=DeploymentTarget.BATCH,\n",
    "        model_name=\"vehicle_insurance_fraud_detector\",\n",
    "        model_version=model_version,\n",
    "        model_artifact_path=model_artifact_path,\n",
    "        deployment_database=database,\n",
    "        deployment_schema=\"MODELS\",\n",
    "        deployment_name=\"vehicle_fraud_predict_udf\",\n",
    "        warehouse=warehouse\n",
    "    )\n",
    "    \n",
    "    # Initialize deployment orchestrator\n",
    "    deployment_orch = DeploymentOrchestrator(session)\n",
    "    \n",
    "    # Deploy model\n",
    "    udf_strategy = WarehouseUDFStrategy(deployment_config)\n",
    "    udf_strategy.set_session(session)\n",
    "    deployment_orch.register_strategy(\"udf\", udf_strategy)\n",
    "    result = deployment_orch.execute(\"udf\")\n",
    "    \n",
    "    if result.status == \"success\":\n",
    "        logger.info(f\"Model deployed as UDF: {result.udf_name}\")\n",
    "        return result.udf_name\n",
    "    else:\n",
    "        logger.error(f\"Deployment failed: {result.error}\")\n",
    "        raise Exception(f\"Deployment failed: {result.error}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88406df9",
   "metadata": {},
   "source": [
    "# Step 8 :Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6f0a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_monitoring(session, udf_name):\n",
    "    \"\"\"Set up monitoring for the deployed model.\"\"\"\n",
    "    logger.info(\"Setting up model monitoring\")\n",
    "    \n",
    "    database = os.getenv(\"SNOWFLAKE_DATABASE\")\n",
    "    \n",
    "    # Create monitoring tables\n",
    "    inference_log_table = f\"{database}.MODELS.INFERENCE_LOG\"\n",
    "    \n",
    "    session.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {inference_log_table} (\n",
    "        inference_id VARCHAR,\n",
    "        model_name VARCHAR,\n",
    "        model_version VARCHAR,\n",
    "        timestamp TIMESTAMP_NTZ,\n",
    "        input VARIANT,\n",
    "        prediction FLOAT,\n",
    "        latency_ms FLOAT,\n",
    "        correlation_id VARCHAR\n",
    "    )\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    # Create monitoring view\n",
    "    monitoring_view = f\"{database}.ANALYTICS.MODEL_PERFORMANCE\"\n",
    "    \n",
    "    session.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {monitoring_view} AS\n",
    "    SELECT\n",
    "        DATE_TRUNC('day', timestamp) AS day,\n",
    "        model_name,\n",
    "        model_version,\n",
    "        COUNT(*) AS inference_count,\n",
    "        AVG(latency_ms) AS avg_latency_ms,\n",
    "        PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY latency_ms) AS p95_latency_ms\n",
    "    FROM {inference_log_table}\n",
    "    GROUP BY 1, 2, 3\n",
    "    ORDER BY 1 DESC\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    logger.info(\"Monitoring setup completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b8d010",
   "metadata": {},
   "source": [
    "# Step 9: Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735910bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_inference(session, limit: int = 10, label_filter: str | None = None):\n",
    "    \"\"\"Batch test model inference using rows from training set.\n",
    "\n",
    "    Args:\n",
    "        session: Snowpark session\n",
    "        limit: number of random rows to score\n",
    "        label_filter: optional filter for label, e.g., 'IS_FRAUD = 1' or 'IS_FRAUD = 0'\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with prediction and actual label\n",
    "    \"\"\"\n",
    "    logger.info(\"Testing model inference (batch)\")\n",
    "    \n",
    "    database = os.getenv(\"SNOWFLAKE_DATABASE\")\n",
    "    udf_name = f\"{database}.MODELS.vehicle_fraud_predict_udf\"\n",
    "    training_table = f\"{database}.FEATURES.TRAINING_DATA\"\n",
    "    \n",
    "    # Determine feature columns directly from table schema\n",
    "    all_cols = session.table(training_table).columns\n",
    "    exclude_cols = [\"IS_FRAUD\", \"SAMPLE_WEIGHT\"]\n",
    "    feature_cols = [c for c in all_cols if c not in exclude_cols]\n",
    "    \n",
    "    # Build OBJECT_CONSTRUCT_KEEP_NULL argument list: 'COL', COL, ...\n",
    "    pairs_sql = \",\\n            \".join([f\"'{c}', {c}\" for c in feature_cols])\n",
    "    \n",
    "    # Optional label filter\n",
    "    where_clause = f\"WHERE {label_filter}\" if label_filter else \"\"\n",
    "    \n",
    "    # Score multiple rows directly in Snowflake AND log to inference table\n",
    "    inference_log_table = f\"{database}.MODELS.INFERENCE_LOG\"\n",
    "    \n",
    "    # First, score and get predictions\n",
    "    test_sql = f\"\"\"\n",
    "    SELECT \n",
    "        TO_DOUBLE({udf_name}(OBJECT_CONSTRUCT_KEEP_NULL(\n",
    "            {pairs_sql}\n",
    "        ))) AS prediction,\n",
    "        IS_FRAUD AS label,\n",
    "        OBJECT_CONSTRUCT_KEEP_NULL({pairs_sql}) AS input_features\n",
    "    FROM {training_table}\n",
    "    {where_clause}\n",
    "    ORDER BY RANDOM()\n",
    "    LIMIT {limit}\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(f\"Scoring {limit} rows with {len(feature_cols)} features\")\n",
    "    rows = session.sql(test_sql).collect()\n",
    "    \n",
    "    # Log each inference to the monitoring table\n",
    "    import uuid\n",
    "    from datetime import datetime\n",
    "    for r in rows:\n",
    "        log_sql = f\"\"\"\n",
    "        INSERT INTO {inference_log_table} \n",
    "        (inference_id, model_name, model_version, timestamp, input, prediction, latency_ms, correlation_id)\n",
    "        VALUES (\n",
    "            '{uuid.uuid4()}',\n",
    "            'vehicle_insurance_fraud_detector',\n",
    "            '1.0.0',\n",
    "            CURRENT_TIMESTAMP(),\n",
    "            PARSE_JSON('{str(r[\"INPUT_FEATURES\"]).replace(\"'\", \"''\")}'),\n",
    "            {r[\"PREDICTION\"]},\n",
    "            0.0,\n",
    "            '{uuid.uuid4()}'\n",
    "        )\n",
    "        \"\"\"\n",
    "        try:\n",
    "            session.sql(log_sql).collect()\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to log inference: {e}\")\n",
    "    results = []\n",
    "    for r in rows:\n",
    "        pred = r[\"PREDICTION\"]\n",
    "        try:\n",
    "            pred_val = float(pred) if pred is not None else None\n",
    "        except Exception:\n",
    "            # Fallback: handle strings in VARIANT\n",
    "            pred_val = float(str(pred)) if pred is not None else None\n",
    "        results.append({\"prediction\": pred_val, \"label\": r[\"LABEL\"]})\n",
    "    \n",
    "    # Basic summary\n",
    "    if results:\n",
    "        positives = sum(1 for r in results if (r[\"prediction\"] is not None and r[\"prediction\"] >= 0.5))\n",
    "        logger.info(f\"Batch inference: {positives}/{len(results)} predicted fraud (>=0.5)\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045a3700",
   "metadata": {},
   "source": [
    "# Step 10: Run all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dd78da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline():\n",
    "    \"\"\"Execute the complete MLOps pipeline.\"\"\"\n",
    "    try:\n",
    "        # Step 1: Setup\n",
    "        session = create_snowflake_session()\n",
    "        setup_infrastructure(session)\n",
    "        \n",
    "        # Step 2: Data Ingestion\n",
    "        csv_path = \"src/datasets/vehicle_insurance_fraud/fraud_oracle.csv\"\n",
    "        table_name = ingest_data(session, csv_path)\n",
    "        \n",
    "        # Step 3: Feature Engineering\n",
    "        feature_store = engineer_features(session)\n",
    "        \n",
    "        # Step 4: Training Dataset Generation\n",
    "        training_table = generate_training_dataset(session, feature_store)\n",
    "        \n",
    "        # Step 5: Model Training\n",
    "        training_result = train_model(session, training_table)\n",
    "        \n",
    "        # Step 6: Model Registration\n",
    "        model_registry, model_version = register_model(session, training_result)\n",
    "        \n",
    "        # Step 7: Model Deployment\n",
    "        udf_name = deploy_model(session, model_registry, model_version)\n",
    "        \n",
    "        # Step 8: Setup Monitoring\n",
    "        setup_monitoring(session, udf_name)\n",
    "        \n",
    "        # Step 9: Test Inference\n",
    "        test_inference(session)\n",
    "        \n",
    "        logger.info(\"Pipeline execution completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline execution failed: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54a95b9",
   "metadata": {},
   "source": [
    "# Internal test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "323ef114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 10:31:15,483 - __main__ - INFO - Creating Snowflake session for account: DYYADUD-EHC01917\n",
      "2025-10-17 10:31:15,484 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.18.0, Python Version: 3.10.18, Platform: macOS-15.1-arm64-arm-64bit\n",
      "2025-10-17 10:31:15,485 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 10:31:18,212 - snowflake.snowpark.session - INFO - Snowpark Session information: \n",
      "\"version\" : 1.40.0,\n",
      "\"python.version\" : 3.10.18,\n",
      "\"python.connector.version\" : 3.18.0,\n",
      "\"python.connector.session.id\" : 32191548682915978,\n",
      "\"os.name\" : Darwin\n",
      "\n",
      "2025-10-17 10:31:18,330 - __main__ - INFO - Connected to Snowflake: Row(CURRENT_WAREHOUSE()='COMPUTE_WH', CURRENT_DATABASE()='ML_CREDIT', CURRENT_SCHEMA()=None)\n",
      "2025-10-17 10:31:18,330 - __main__ - INFO - Setting up Snowflake infrastructure\n",
      "2025-10-17 10:31:19,826 - __main__ - INFO - Infrastructure setup completed\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Setup\n",
    "session = create_snowflake_session()\n",
    "setup_infrastructure(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ebede89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 10:43:19,995 - __main__ - INFO - Ingesting data from src/datasets/vehicle_insurance_fraud/fraud_oracle.csv\n",
      "2025-10-17 10:43:20,030 - __main__ - INFO - Loaded CSV with 15420 rows and 33 columns\n",
      "2025-10-17 10:43:26,360 - __main__ - INFO - Ingested 15420 rows into ML_CREDIT.RAW_DATA.INSURANCE_CLAIMS\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Data Ingestion\n",
    "csv_path = \"src/datasets/vehicle_insurance_fraud/fraud_oracle.csv\"\n",
    "table_name = ingest_data(session, csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d51d22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 10:31:19,845 - __main__ - INFO - Engineering features for fraud detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 10:31:20 - INFO - snowflake_ml_template.feature_store.core.store - Initialized Feature Store: ML_CREDIT.FEATURES [correlation_id=4b3abd36-5be0-44b3-852c-88b07935c21e]\n",
      "2025-10-17 10:31:20,157 - snowflake_ml_template.feature_store.core.store - INFO - Initialized Feature Store: ML_CREDIT.FEATURES\n",
      "2025-10-17 10:31:20,159 - __main__ - INFO - Performing data quality checks\n",
      "2025-10-17 10:31:20 - INFO - snowflake_ml_template.feature_store.core.store - Registered entity: POLICY with join keys: ['POLICYNUMBER'] [correlation_id=4b3abd36-5be0-44b3-852c-88b07935c21e]\n",
      "2025-10-17 10:31:20,465 - snowflake_ml_template.feature_store.core.store - INFO - Registered entity: POLICY with join keys: ['POLICYNUMBER']\n",
      "2025-10-17 10:31:20 - INFO - snowflake_ml_template.feature_store.core.store - Registered entity: CLAIM with join keys: ['POLICYNUMBER', 'MONTH', 'WEEKOFMONTH'] [correlation_id=4b3abd36-5be0-44b3-852c-88b07935c21e]\n",
      "2025-10-17 10:31:20,616 - snowflake_ml_template.feature_store.core.store - INFO - Registered entity: CLAIM with join keys: ['POLICYNUMBER', 'MONTH', 'WEEKOFMONTH']\n",
      "2025-10-17 10:31:20,617 - __main__ - INFO - Creating temporal features\n",
      "2025-10-17 10:31:20,620 - __main__ - INFO - Creating vehicle features\n",
      "2025-10-17 10:31:20,623 - __main__ - INFO - Creating demographic features\n",
      "2025-10-17 10:31:20,625 - __main__ - INFO - Creating claim risk factors\n",
      "2025-10-17 10:31:20,628 - __main__ - INFO - Creating policy aggregation features\n",
      "2025-10-17 10:31:20,630 - __main__ - INFO - Joining all feature sets\n",
      "2025-10-17 10:31:22,146 - __main__ - INFO - Creating interaction features\n",
      "2025-10-17 10:31:22,266 - __main__ - INFO - Calculating class weights for imbalanced data\n",
      "2025-10-17 10:31:22,448 - __main__ - INFO - Fraud ratio: 0.0599 (923/15420)\n",
      "2025-10-17 10:31:22,451 - __main__ - INFO - Registering feature views in Feature Store\n",
      "2025-10-17 10:31:24 - INFO - snowflake_ml_template.feature_store.core.store - Created table: FEATURE_VIEW_fraud_detection_features_V1_0_0 [correlation_id=4b3abd36-5be0-44b3-852c-88b07935c21e]\n",
      "2025-10-17 10:31:24,090 - snowflake_ml_template.feature_store.core.store - INFO - Created table: FEATURE_VIEW_fraud_detection_features_V1_0_0\n",
      "2025-10-17 10:31:24 - INFO - snowflake_ml_template.feature_store.core.store - Registered FeatureView: fraud_detection_features (Snowflake-managed) [correlation_id=4b3abd36-5be0-44b3-852c-88b07935c21e]\n",
      "2025-10-17 10:31:24,337 - snowflake_ml_template.feature_store.core.store - INFO - Registered FeatureView: fraud_detection_features (Snowflake-managed)\n",
      "2025-10-17 10:31:25 - INFO - snowflake_ml_template.feature_store.core.store - Created table: FEATURE_VIEW_policy_level_features_V1_0_0 [correlation_id=4b3abd36-5be0-44b3-852c-88b07935c21e]\n",
      "2025-10-17 10:31:25,720 - snowflake_ml_template.feature_store.core.store - INFO - Created table: FEATURE_VIEW_policy_level_features_V1_0_0\n",
      "2025-10-17 10:31:25 - INFO - snowflake_ml_template.feature_store.core.store - Registered FeatureView: policy_level_features (Snowflake-managed) [correlation_id=4b3abd36-5be0-44b3-852c-88b07935c21e]\n",
      "2025-10-17 10:31:25,848 - snowflake_ml_template.feature_store.core.store - INFO - Registered FeatureView: policy_level_features (Snowflake-managed)\n",
      "2025-10-17 10:31:25,849 - __main__ - INFO - Feature engineering completed: 36 features created\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Feature Engineering\n",
    "feature_store = engineer_features(session)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d819776a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 10:34:37,529 - __main__ - INFO - Generating training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 10:34:40,523 - __main__ - INFO - Generated training dataset with 15420 rows\n",
      "2025-10-17 10:34:40,524 - __main__ - INFO - Fraud cases: 923, Non-fraud: 14497\n",
      "2025-10-17 10:34:40,524 - __main__ - INFO - Fraud ratio: 0.0599\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Training Dataset Generation\n",
    "training_table = generate_training_dataset(session, feature_store)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78b67e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ML_CREDIT.FEATURES.TRAINING_DATA'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92e2b58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 10:34:41,256 - __main__ - INFO - Training fraud detection model\n",
      "2025-10-17 10:34:45,603 - __main__ - INFO - Calculated scale_pos_weight: 15.71\n",
      "2025-10-17 10:34:45 - INFO - snowflake_ml_template.training.orchestrator - Registered trainer: lightgbm [correlation_id=c9acc57b-7909-4aab-8271-137792ffed8d]\n",
      "2025-10-17 10:34:45,604 - snowflake_ml_template.training.orchestrator - INFO - Registered trainer: lightgbm\n",
      "2025-10-17 10:34:45,605 - __main__ - INFO - Starting model training with LightGBM...\n",
      "2025-10-17 10:34:47 - INFO - snowflake_ml_template.training.orchestrator - Training completed: lightgbm [correlation_id=c9acc57b-7909-4aab-8271-137792ffed8d, status=success]\n",
      "2025-10-17 10:34:47,518 - snowflake_ml_template.training.orchestrator - INFO - Training completed: lightgbm\n",
      "2025-10-17 10:34:47,519 - __main__ - INFO - Training successful: /var/folders/0l/1xhjgnkn3f56_jts1rj61xl80000gn/T/lightgbm_model_20251017_153447.joblib\n",
      "2025-10-17 10:34:47,519 - __main__ - INFO - Model trained with class imbalance handling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingResult(status='success', strategy=<TrainingStrategy.SINGLE_NODE: 'single_node'>, framework=<MLFramework.LIGHTGBM: 'lightgbm'>, model_artifact_path='/var/folders/0l/1xhjgnkn3f56_jts1rj61xl80000gn/T/lightgbm_model_20251017_153447.joblib', metrics={}, best_epoch=0, total_epochs=0, training_samples=0, validation_samples=0, test_samples=0, start_time=datetime.datetime(2025, 10, 17, 15, 34, 45, 606102), end_time=datetime.datetime(2025, 10, 17, 15, 34, 47, 518526), duration_seconds=1.912424, error=None, metadata={})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Model Training\n",
    "training_result = train_model(session, training_table)\n",
    "training_result      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4615763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingResult(status='success', strategy=<TrainingStrategy.SINGLE_NODE: 'single_node'>, framework=<MLFramework.LIGHTGBM: 'lightgbm'>, model_artifact_path='/var/folders/0l/1xhjgnkn3f56_jts1rj61xl80000gn/T/lightgbm_model_20251017_153447.joblib', metrics={}, best_epoch=0, total_epochs=0, training_samples=0, validation_samples=0, test_samples=0, start_time=datetime.datetime(2025, 10, 17, 15, 34, 45, 606102), end_time=datetime.datetime(2025, 10, 17, 15, 34, 47, 518526), duration_seconds=1.912424, error=None, metadata={})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23b39086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 10:34:50,968 - __main__ - INFO - Registering model in registry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 10:34:52 - INFO - snowflake_ml_template.registry.manager - Registered model version: vehicle_insurance_fraud_detector v1.0.0 (dev) [correlation_id=2b387c9b-80e2-4d82-b179-c413964602dd, model=vehicle_insurance_fraud_detector, version=1.0.0, stage=dev]\n",
      "2025-10-17 10:34:52,694 - snowflake_ml_template.registry.manager - INFO - Registered model version: vehicle_insurance_fraud_detector v1.0.0 (dev)\n",
      "2025-10-17 10:34:52,695 - __main__ - INFO - Model registered: vehicle_insurance_fraud_detector v1.0.0\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Model Registration\n",
    "model_registry, model_version = register_model(session, training_result)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "379e10fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 10:35:18,273 - __main__ - INFO - Deploying model as Warehouse UDF\n",
      "2025-10-17 10:35:18,274 - __main__ - INFO - Deploying model from: /var/folders/0l/1xhjgnkn3f56_jts1rj61xl80000gn/T/lightgbm_model_20251017_153447.joblib\n",
      "2025-10-17 10:35:18 - INFO - snowflake_ml_template.deployment.orchestrator - Registered deployment strategy: udf [correlation_id=ba0f5236-e325-40be-b8f2-77661760ecaf]\n",
      "2025-10-17 10:35:18,274 - snowflake_ml_template.deployment.orchestrator - INFO - Registered deployment strategy: udf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 10:35:19 - INFO - snowflake_ml_template.deployment.strategies.warehouse_udf - Uploaded model to @ML_CREDIT.MODELS.ML_MODELS_STAGE/lightgbm_model_20251017_153447.joblib [correlation_id=ad66895f-df21-4f9a-8a01-1e7860a25971]\n",
      "2025-10-17 10:35:19,115 - snowflake_ml_template.deployment.strategies.warehouse_udf - INFO - Uploaded model to @ML_CREDIT.MODELS.ML_MODELS_STAGE/lightgbm_model_20251017_153447.joblib\n",
      "2025-10-17 10:35:23 - INFO - snowflake_ml_template.deployment.orchestrator - Deployment completed: udf [correlation_id=ba0f5236-e325-40be-b8f2-77661760ecaf, status=success]\n",
      "2025-10-17 10:35:23,458 - snowflake_ml_template.deployment.orchestrator - INFO - Deployment completed: udf\n",
      "2025-10-17 10:35:23,459 - __main__ - INFO - Model deployed as UDF: ML_CREDIT.MODELS.vehicle_fraud_predict_udf\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Model Deployment\n",
    "udf_name = deploy_model(session, model_registry, model_version, training_result)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9040cf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 10:35:33,702 - __main__ - INFO - Setting up model monitoring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 10:35:34,040 - __main__ - INFO - Monitoring setup completed\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Setup Monitoring\n",
    "setup_monitoring(session, udf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0650639f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 10:39:23,515 - __main__ - INFO - Testing model inference (batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 10:39:23,758 - __main__ - INFO - Scoring 10 rows with 32 features\n",
      "2025-10-17 10:39:27,348 - __main__ - INFO - Batch inference: 1/10 predicted fraud (>=0.5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prediction': 0.0, 'label': 0},\n",
       " {'prediction': 1.0, 'label': 1},\n",
       " {'prediction': 0.0, 'label': 0},\n",
       " {'prediction': 0.0, 'label': 0},\n",
       " {'prediction': 0.0, 'label': 0},\n",
       " {'prediction': 0.0, 'label': 0},\n",
       " {'prediction': 0.0, 'label': 0},\n",
       " {'prediction': 0.0, 'label': 0},\n",
       " {'prediction': 0.0, 'label': 0},\n",
       " {'prediction': 0.0, 'label': 0}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 9: Test Inference\n",
    "test_inference(session)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
