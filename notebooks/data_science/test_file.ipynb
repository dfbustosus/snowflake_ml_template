{
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "lastEditStatus": {
   "notebookId": "fay5wtu2qax5girxwfwa",
   "authorId": "1325166941769",
   "authorName": "DAVIDUSTA",
   "authorEmail": "davidbustosusta@gmail.com",
   "sessionId": "df475b89-9147-4fa1-af52-4f302c121681",
   "lastEditTime": 1761691385101
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e18c640-1cb3-4302-9ece-9c4ac055203e",
   "metadata": {
    "name": "cell1",
    "language": "python"
   },
   "outputs": [],
   "source": "# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e4f2f9a-73f6-4b13-b6e9-90a7a91dd596",
   "metadata": {
    "name": "cell2",
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "import logging\nfrom colorama import Fore, Style, init\n\n# Initialize colorama for cross-platform color support\ninit(autoreset=True)\n\n# Color mapping by log level\nLOG_COLORS = {\n    logging.DEBUG: Fore.CYAN,\n    logging.INFO: Fore.GREEN,\n    logging.WARNING: Fore.YELLOW,\n    logging.ERROR: Fore.RED,\n    logging.CRITICAL: Fore.MAGENTA\n}\n\nclass ColorFormatter(logging.Formatter):\n    def format(self, record):\n        color = LOG_COLORS.get(record.levelno, \"\")\n        message = super().format(record)\n        return f\"{color}{message}{Style.RESET_ALL}\"\n\ndef get_logger(name=\"AppLogger\", level=logging.INFO):\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n\n    if not logger.handlers:\n        handler = logging.StreamHandler()\n        formatter = ColorFormatter(\"[%(asctime)s] [%(levelname)s] %(message)s\", datefmt=\"%H:%M:%S\")\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n\n    return logger\n\nlogger = get_logger(\"DemoLogger\", logging.DEBUG)"
  },
  {
   "cell_type": "markdown",
   "id": "c471d796-3adb-4e22-92a4-d41ec8ba569d",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "# Extract"
  },
  {
   "cell_type": "code",
   "id": "c40b748c-0ef5-45ba-b345-6712ffb1b1b3",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": "# Import python packages\nimport os\nfrom snowflake.snowpark import Session\nimport pandas as pd\n\nsession = get_active_session()\n\ntable_name = \"ML_CREDIT.RAW_DATA.INSURANCE_CLAIMS\"\nclaims = session.table(table_name)\nclaims.show()\n\n\nlogger.info(f\"Total records: {claims.count()}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7e5f9dd4-a63d-4156-884e-990c8b0df6ec",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": "# Transform"
  },
  {
   "cell_type": "code",
   "id": "32e399c5-4e10-4b06-8e96-941c23165708",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import avg, col,when, count, lit  # noqa: E402\nfrom snowflake.snowpark.functions import max as max_  # noqa: E402\nfrom snowflake.snowpark.functions import sum as sum_  # noqa: E402\n\n\n# 0. Fix AGE (likely missing value)\nclaims = claims.with_column(\n    \"AGE\",\n    when((col(\"AGE\") <= 0) | (col(\"AGE\") > 120), lit(None)).otherwise(\n        col(\"AGE\").cast(\"int\")\n    ),\n)\n# 1. TEMPORAL FEATURES - Critical for fraud detection\ntemporal_features = claims.select(\n    col(\"POLICYNUMBER\"),\n    col(\"MONTH\"),\n    col(\"WEEKOFMONTH\"),\n    # Convert categorical time ranges to numeric\n    when(col(\"DAYS_POLICY_CLAIM\") == \"more than 30\", 35)\n    .when(col(\"DAYS_POLICY_CLAIM\") == \"15 to 30\", 22)\n    .when(col(\"DAYS_POLICY_CLAIM\") == \"8 to 15\", 11)\n    .when(col(\"DAYS_POLICY_CLAIM\") == \"1 to 7\", 4)\n    .otherwise(0)\n    .alias(\"DAYS_TO_CLAIM_NUM\"),\n    when(col(\"DAYS_POLICY_ACCIDENT\") == \"more than 30\", 35)\n    .when(col(\"DAYS_POLICY_ACCIDENT\") == \"15 to 30\", 22)\n    .when(col(\"DAYS_POLICY_ACCIDENT\") == \"8 to 15\", 11)\n    .when(col(\"DAYS_POLICY_ACCIDENT\") == \"1 to 7\", 4)\n    .otherwise(0)\n    .alias(\"POLICY_AGE_AT_ACCIDENT\"),\n    # Suspicious if claim month differs from accident month\n    when(col(\"MONTH\") != col(\"MONTHCLAIMED\"), 1)\n    .otherwise(0)\n    .alias(\"MONTH_MISMATCH\"),\n    # Suspicious if day of week differs\n    when(col(\"DAYOFWEEK\") != col(\"DAYOFWEEKCLAIMED\"), 1)\n    .otherwise(0)\n    .alias(\"DAY_MISMATCH\"),\n)\n\n\n# 2. VEHICLE FEATURES\nvehicle_features = claims.select(\n    col(\"POLICYNUMBER\"),\n    col(\"MONTH\"),\n    col(\"WEEKOFMONTH\"),\n    # Vehicle age numeric\n    when(col(\"AGEOFVEHICLE\") == \"new\", 0)\n    .when(col(\"AGEOFVEHICLE\") == \"1 year\", 1)\n    .when(col(\"AGEOFVEHICLE\") == \"2 years\", 2)\n    .when(col(\"AGEOFVEHICLE\") == \"3 years\", 3)\n    .when(col(\"AGEOFVEHICLE\") == \"4 years\", 4)\n    .when(col(\"AGEOFVEHICLE\") == \"5 years\", 5)\n    .when(col(\"AGEOFVEHICLE\") == \"6 years\", 6)\n    .when(col(\"AGEOFVEHICLE\") == \"7 years\", 7)\n    .when(col(\"AGEOFVEHICLE\") == \"more than 7\", 9)\n    .otherwise(None)\n    .alias(\"VEHICLE_AGE_NUM\"),\n    # Vehicle price midpoint\n    when(col(\"VEHICLEPRICE\") == \"less than 20000\", 15000)\n    .when(col(\"VEHICLEPRICE\") == \"20000 to 29000\", 24500)\n    .when(col(\"VEHICLEPRICE\") == \"30000 to 39000\", 34500)\n    .when(col(\"VEHICLEPRICE\") == \"40000 to 59000\", 49500)\n    .when(col(\"VEHICLEPRICE\") == \"60000 to 69000\", 64500)\n    .when(col(\"VEHICLEPRICE\") == \"more than 69000\", 80000)\n    .otherwise(None)\n    .alias(\"VEHICLE_PRICE_NUM\"),\n    # Risk factors based on insurance industry data\n    when(col(\"VEHICLECATEGORY\") == \"Sport\", 3)\n    .when(col(\"VEHICLECATEGORY\") == \"Utility\", 2)\n    .when(col(\"VEHICLECATEGORY\") == \"Sedan\", 1)\n    .otherwise(1)\n    .alias(\"VEHICLE_RISK\"),\n)\n\n# 3. DEMOGRAPHIC FEATURES\ndemographic_features = claims.select(\n    col(\"POLICYNUMBER\"),\n    col(\"MONTH\"),\n    col(\"WEEKOFMONTH\"),\n    # Age risk (young and elderly are higher risk)\n    when(col(\"AGE\") < 25, 3)\n    .when(col(\"AGE\").between(25, 35), 2)\n    .when(col(\"AGE\").between(36, 60), 1)\n    .when(col(\"AGE\") > 60, 2)\n    .otherwise(2)\n    .alias(\"AGE_RISK\"),\n    # Binary encodings\n    when(col(\"SEX\") == \"Male\", 1).otherwise(0).alias(\"IS_MALE\"),\n    when(col(\"MARITALSTATUS\") == \"Single\", 1).otherwise(0).alias(\"IS_SINGLE\"),\n    # Driver rating (already numeric)\n    col(\"DRIVERRATING\"),\n    # Policyholder age midpoint\n    when(col(\"AGEOFPOLICYHOLDER\") == \"16 to 17\", 16.5)\n    .when(col(\"AGEOFPOLICYHOLDER\") == \"18 to 20\", 19)\n    .when(col(\"AGEOFPOLICYHOLDER\") == \"21 to 25\", 23)\n    .when(col(\"AGEOFPOLICYHOLDER\") == \"26 to 30\", 28)\n    .when(col(\"AGEOFPOLICYHOLDER\") == \"31 to 35\", 33)\n    .when(col(\"AGEOFPOLICYHOLDER\") == \"36 to 40\", 38)\n    .when(col(\"AGEOFPOLICYHOLDER\") == \"41 to 50\", 45.5)\n    .when(col(\"AGEOFPOLICYHOLDER\") == \"51 to 65\", 58)\n    .when(col(\"AGEOFPOLICYHOLDER\") == \"over 65\", 72)\n    .otherwise(None)\n    .alias(\"POLICYHOLDER_AGE\"),\n)\n\n # 4. CLAIM RISK FACTORS - Most important for fraud detection\n\nclaim_risk_features = claims.select(\n    col(\"POLICYNUMBER\"),\n    col(\"MONTH\"),\n    col(\"WEEKOFMONTH\"),\n    # Fault\n    when(col(\"FAULT\") == \"Policy Holder\", 1)\n    .otherwise(0)\n    .alias(\"POLICYHOLDER_FAULT\"),\n    # Deductible\n    col(\"DEDUCTIBLE\"),\n    # Past claims (strong fraud indicator)\n    when(col(\"PASTNUMBEROFCLAIMS\") == \"none\", 0)\n    .when(col(\"PASTNUMBEROFCLAIMS\") == \"1\", 1)\n    .when(col(\"PASTNUMBEROFCLAIMS\") == \"2 to 4\", 3)\n    .when(col(\"PASTNUMBEROFCLAIMS\") == \"more than 4\", 6)\n    .otherwise(0)\n    .alias(\"PAST_CLAIMS\"),\n    # Documentation flags (strong fraud indicators)\n    when(col(\"POLICEREPORTFILED\") == \"No\", 1)\n    .otherwise(0)\n    .alias(\"NO_POLICE_REPORT\"),\n    when(col(\"WITNESSPRESENT\") == \"No\", 1).otherwise(0).alias(\"NO_WITNESS\"),\n    # Claim supplements\n    when(col(\"NUMBEROFSUPPLIMENTS\") == \"none\", 0)\n    .when(col(\"NUMBEROFSUPPLIMENTS\") == \"1 to 2\", 1.5)\n    .when(col(\"NUMBEROFSUPPLIMENTS\") == \"3 to 5\", 4)\n    .when(col(\"NUMBEROFSUPPLIMENTS\") == \"more than 5\", 7)\n    .otherwise(0)\n    .alias(\"SUPPLEMENTS\"),\n    # Address change (fraud red flag)\n    when(col(\"ADDRESSCHANGE_CLAIM\") == \"1 year\", 1)\n    .when(col(\"ADDRESSCHANGE_CLAIM\") == \"2 to 3 years\", 0.5)\n    .when(col(\"ADDRESSCHANGE_CLAIM\") == \"4 to 8 years\", 0.2)\n    .when(col(\"ADDRESSCHANGE_CLAIM\") == \"no change\", 0)\n    .otherwise(0)\n    .alias(\"ADDRESS_CHANGE\"),\n    # Location and agent\n    when(col(\"ACCIDENTAREA\") == \"Urban\", 1).otherwise(0).alias(\"URBAN_ACCIDENT\"),\n    when(col(\"AGENTTYPE\") == \"External\", 1).otherwise(0).alias(\"EXTERNAL_AGENT\"),\n    # Target\n    col(\"FRAUDFOUND_P\").alias(\"IS_FRAUD\"),\n)\n\n# 5. POLICY AGGREGATIONS - Historical behavior\n\npolicy_agg = claims.group_by(\"POLICYNUMBER\").agg(\n    count(col(\"POLICYNUMBER\")).alias(\"TOTAL_CLAIMS_POLICY\"),\n    sum_(col(\"FRAUDFOUND_P\")).alias(\"FRAUD_COUNT_POLICY\"),\n    avg(col(\"DEDUCTIBLE\")).alias(\"AVG_DEDUCTIBLE_POLICY\"),\n    avg(col(\"DRIVERRATING\")).alias(\"AVG_RATING_POLICY\"),\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f1c69d03-60a6-48c1-b3de-035b5348896d",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "# ======================================================================\n# JOIN ALL FEATURES\n# ======================================================================\nlogger.info(\"Joining all feature sets\")\n\n# Use USING clause to avoid duplicate columns\nall_features = claim_risk_features.join(\n    temporal_features, [\"POLICYNUMBER\", \"MONTH\", \"WEEKOFMONTH\"], \"left\"\n)\n\nall_features = all_features.join(\n    vehicle_features, [\"POLICYNUMBER\", \"MONTH\", \"WEEKOFMONTH\"], \"left\"\n)\n\nall_features = all_features.join(\n    demographic_features, [\"POLICYNUMBER\", \"MONTH\", \"WEEKOFMONTH\"], \"left\"\n)\n\nall_features = all_features.join(policy_agg, \"POLICYNUMBER\", \"left\")\n\n# ======================================================================\n# INTERACTION FEATURES - Capture complex fraud patterns\n# ======================================================================\nlogger.info(\"Creating interaction features\")\n\nfinal_features = all_features.select(\n    \"*\",\n    # Quick claim + no police report = very suspicious\n    (col(\"DAYS_TO_CLAIM_NUM\") * col(\"NO_POLICE_REPORT\")).alias(\"QUICK_NO_POLICE\"),\n    # Vehicle depreciation vs price\n    (col(\"VEHICLE_AGE_NUM\") * col(\"VEHICLE_PRICE_NUM\") / 10000).alias(\n        \"VEHICLE_DEPRECIATION\"\n    ),\n    # External agent in urban area\n    (col(\"EXTERNAL_AGENT\") * col(\"URBAN_ACCIDENT\")).alias(\"EXTERNAL_URBAN\"),\n    # Address change with past claims\n    (col(\"ADDRESS_CHANGE\") * col(\"PAST_CLAIMS\")).alias(\"ADDRESS_PAST_CLAIMS\"),\n    # Young driver with sport vehicle\n    (when(col(\"AGE_RISK\") == 3, 1).otherwise(0) * col(\"VEHICLE_RISK\")).alias(\n        \"YOUNG_SPORT\"\n    ),\n    # New policy with claim\n    (\n        when(col(\"POLICY_AGE_AT_ACCIDENT\") < 15, 1).otherwise(0) * col(\"DEDUCTIBLE\")\n    ).alias(\"NEW_POLICY_CLAIM\"),\n    # No documentation (police + witness)\n    (col(\"NO_POLICE_REPORT\") * col(\"NO_WITNESS\")).alias(\"NO_DOCUMENTATION\"),\n)\n\n# ======================================================================\n# CLASS IMBALANCE HANDLING\n# ======================================================================\nlogger.info(\"Calculating class weights for imbalanced data\")\n\nfraud_stats = session.sql(\n    f\"\"\"\n    SELECT\n        SUM(CASE WHEN FRAUDFOUND_P = 1 THEN 1 ELSE 0 END) AS FRAUD_COUNT,\n        COUNT(*) AS TOTAL_COUNT\n    FROM ML_CREDIT.RAW_DATA.INSURANCE_CLAIMS\n\"\"\"\n).collect()[0]\n\nfraud_count = fraud_stats[\"FRAUD_COUNT\"]\ntotal_count = fraud_stats[\"TOTAL_COUNT\"]\nfraud_ratio = fraud_count / total_count\n\nlogger.info(f\"Fraud ratio: {fraud_ratio:.4f} ({fraud_count}/{total_count})\")\n\n# Add sample weights (inverse of class frequency)\nweighted_features = final_features.select(\n    \"*\",\n    when(col(\"IS_FRAUD\") == 1, (1 - fraud_ratio) / fraud_ratio)\n    .otherwise(1.0)\n    .alias(\"SAMPLE_WEIGHT\"),\n)\n\n# ======================================================================\n# REGISTER FEATURE VIEWS\n# ======================================================================\nlogger.info(\"Registering feature views in Feature Store\")\nweighted_features.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8226908c-d8e5-48e4-9d64-053f4c1ada43",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": "# Model"
  },
  {
   "cell_type": "code",
   "id": "e017dde7-1c2a-48c1-bf94-710584bcad04",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "# ---------------------------\n# CELL 2 - Helper functions\n# ---------------------------\n\ndef prepare_data_from_snowpark(df_snp, exclude_cols=None, target_col='IS_FRAUD', weight_col='SAMPLE_WEIGHT'):\n    \"\"\"Convert Snowpark DataFrame to pandas and prepare X, y, sample_weight.\n    df_snp: snowpark.DataFrame or pandas.DataFrame\n    \"\"\"\n    if exclude_cols is None:\n        exclude_cols = [\"POLICYNUMBER\", \"MONTH\", \"WEEKOFMONTH\"]\n\n    # If Snowpark DataFrame, convert\n    if hasattr(df_snp, 'to_pandas'):\n        df = df_snp.to_pandas()\n    else:\n        df = df_snp.copy()\n\n    # Drop rows with missing target\n    df = df[df[target_col].notna()].reset_index(drop=True)\n\n    # Identify features\n    feature_cols = [c for c in df.columns if c not in exclude_cols and c != target_col and c != weight_col]\n\n    X = df[feature_cols]\n    y = df[target_col].astype(int)\n    sample_weight = df[weight_col] if weight_col in df.columns else None\n\n    return X, y, sample_weight, df\n\n\ndef make_oof_predictions(clf_builder, X, y, sample_weight=None, n_splits=5, scaler_needed=True):\n    \"\"\"Run Stratified K-Fold and return out-of-fold predicted probabilities and fitted models list.\n    clf_builder: function() -> sklearn-like estimator (fresh instance each call)\n    Returns: oof_proba (np.array), models (list), cv_metrics (dict)\n    \"\"\"\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_proba = np.zeros(len(y))\n    models = []\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        sw_train = sample_weight.iloc[train_idx] if sample_weight is not None else None\n\n        if scaler_needed:\n            scaler = StandardScaler()\n            X_train_scaled = scaler.fit_transform(X_train)\n            X_val_scaled = scaler.transform(X_val)\n        else:\n            X_train_scaled, X_val_scaled = X_train.values, X_val.values\n            scaler = None\n\n        model = clf_builder()\n        # Some sklearn wrappers accept sample_weight in fit\n        fit_kwargs = {'X': X_train_scaled, 'y': y_train}\n        try:\n            if sw_train is not None:\n                model.fit(X_train_scaled, y_train, sample_weight=sw_train)\n            else:\n                model.fit(X_train_scaled, y_train)\n        except TypeError:\n            # fallback for some wrappers with different API\n            model.fit(X_train_scaled, y_train)\n\n        # Predict proba for val\n        try:\n            proba = model.predict_proba(X_val_scaled)[:, 1]\n        except Exception:\n            # Some models may not have predict_proba; fall back to decision_function\n            try:\n                scores = model.decision_function(X_val_scaled)\n                proba = 1 / (1 + np.exp(-scores))\n            except Exception:\n                proba = model.predict(X_val_scaled)\n\n        oof_proba[val_idx] = proba\n\n        # store model + scaler\n        models.append({'model': model, 'scaler': scaler})\n\n        logger.info(f\"Fold {fold+1}/{n_splits} done\")\n\n    # Compute CV AUC\n    auc = roc_auc_score(y, oof_proba)\n    return oof_proba, models, {'auc': auc}\n\n\ndef compute_metrics(y_true, proba, threshold=0.5):\n    y_pred = (proba >= threshold).astype(int)\n    return {\n        'auc': float(roc_auc_score(y_true, proba)),\n        'accuracy': float(accuracy_score(y_true, y_pred)),\n        'precision': float(precision_score(y_true, y_pred, zero_division=0)),\n        'recall': float(recall_score(y_true, y_pred, zero_division=0)),\n        'f1': float(f1_score(y_true, y_pred, zero_division=0)),\n        'confusion_matrix': confusion_matrix(y_true, y_pred).tolist()\n    }\n\n\ndef plot_calibration(y_true, proba, n_bins=10, title=None):\n    prob_true, prob_pred = calibration_curve(y_true, proba, n_bins=n_bins)\n    fig, ax = plt.subplots(figsize=(6, 4))\n    ax.plot(prob_pred, prob_true, marker='o', linewidth=1)\n    ax.plot([0, 1], [0, 1], linestyle='--')\n    ax.set_xlabel('Predicted probability')\n    ax.set_ylabel('Observed frequency')\n    if title:\n        ax.set_title(title)\n    return fig\n\ndef plot_rank_order(y_true, proba, n_bins=10, title=None):\n    df = pd.DataFrame({'y': y_true, 'proba': proba})\n    df['decile'] = pd.qcut(df['proba'].rank(method='first'), q=n_bins, labels=False)\n    dec = df.groupby('decile').agg({'y': ['sum', 'count'], 'proba': 'mean'})\n    dec.columns = ['fraud_count', 'total', 'avg_proba']\n    dec = dec.sort_index(ascending=False)  # highest score first\n    dec['fraud_rate'] = dec['fraud_count'] / dec['total']\n    fig, ax = plt.subplots(figsize=(6, 4))\n    ax.bar(range(1, n_bins+1), dec['fraud_rate'])\n    ax.set_xlabel('Decile (1 = highest scores)')\n    ax.set_ylabel('Fraud rate')\n    if title:\n        ax.set_title(title)\n    return fig, dec\n\n\ndef compute_business_metric(df_full, proba, model_name, threshold=0.5, booking_col='BOOKED'):\n    \"\"\"Compute a sample business metric: booking rate overall and for predicted negatives/positives.\n    If booking_col not present, function will return None and skip.\n    \"\"\"\n    df = df_full.copy()\n    df['_proba'] = proba\n    df['_pred'] = (df['_proba'] >= threshold).astype(int)\n\n    if booking_col not in df.columns:\n        logger.warning(f\"Business column '{booking_col}' not found in data. Skipping business metric.\")\n        return None\n\n    overall = df[booking_col].mean()\n    rate_pred_pos = df[df['_pred'] == 1][booking_col].mean()\n    rate_pred_neg = df[df['_pred'] == 0][booking_col].mean()\n\n    return {\n        'model': model_name,\n        'overall_booking_rate': float(overall),\n        'booking_rate_pred_pos': float(rate_pred_pos) if not np.isnan(rate_pred_pos) else None,\n        'booking_rate_pred_neg': float(rate_pred_neg) if not np.isnan(rate_pred_neg) else None,\n        'n_pred_pos': int((df['_pred'] == 1).sum()),\n        'n_pred_neg': int((df['_pred'] == 0).sum())\n    }",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b3f62a1d-d227-4858-89db-bb44c27b58ec",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": "# Unified modeling notebook: Logistic Regression, LightGBM, XGBoost + CV + Model Registry + Streamlit dashboard\n# Author: Generated for you\n# Usage: Run as Jupyter notebook cells. Streamlit app is included at the bottom as a separate runnable block.\n\n# ---------------------------\n# CELL 1 - Imports & config\n# ---------------------------\nimport os\nimport json\nimport tempfile\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (roc_auc_score, accuracy_score, precision_score,\n                             recall_score, f1_score, confusion_matrix)\nfrom sklearn.calibration import calibration_curve\n\n# LightGBM & XGBoost\ntry:\n    import lightgbm as lgb\nexcept Exception:\n    lgb = None\n\ntry:\n    import xgboost as xgb\nexcept Exception:\n    xgb = None\n\n# Plotting\nimport matplotlib.pyplot as plt\n\n# Snowflake Snowpark (user must have snowpark installed and active session configured)\nfrom snowflake.snowpark.context import get_active_session\n\n# Persist models\nimport joblib\n\n# Logger (simple)\nimport logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e8f5e746-14e3-4f14-bad7-c25c7de99591",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n)\nimport joblib, tempfile, os, json\nfrom datetime import datetime\nimport numpy as np\n\ndef ks_statistic(y_true, y_proba):\n    \"\"\"Compute Kolmogorov–Smirnov statistic.\"\"\"\n    data = pd.DataFrame({'y': y_true, 'proba': y_proba})\n    data = data.sort_values('proba', ascending=False)\n    data['cum_pos'] = np.cumsum(data['y'])\n    data['cum_neg'] = np.cumsum(1 - data['y'])\n    total_pos = data['y'].sum()\n    total_neg = len(data) - total_pos\n    data['cum_pos_rate'] = data['cum_pos'] / total_pos if total_pos > 0 else 0\n    data['cum_neg_rate'] = data['cum_neg'] / total_neg if total_neg > 0 else 0\n    data['ks'] = np.abs(data['cum_pos_rate'] - data['cum_neg_rate'])\n    return data['ks'].max()\n\ndef train_and_register_models(\n    session,\n    weighted_features,\n    target_col='IS_FRAUD',\n    weight_col='SAMPLE_WEIGHT',\n    exclude_cols=None,\n    models_to_run=None,\n    db_for_registry='ML_CREDIT',\n    models_stage_name='MODELS.ML_MODELS_STAGE',\n    registry_table_name='MODELS.MODEL_REGISTRY_COMPARISON',\n    metrics_val_table_name='ANALYTICS.MODEL_METRICS_VAL',\n    metrics_test_table_name='ANALYTICS.MODEL_METRICS_TEST',\n    booking_col='BOOKED'\n):\n    if models_to_run is None:\n        models_to_run = ['logistic', 'lgbm', 'xgb']\n\n    X, y, sample_weight, df_full = prepare_data_from_snowpark(weighted_features, exclude_cols, target_col, weight_col)\n\n    # -----------------------------------\n    # Train/Val/Test Split\n    # -----------------------------------\n    X_train, X_temp, y_train, y_temp, w_train, w_temp = train_test_split(\n        X, y, sample_weight, test_size=0.3, stratify=y, random_state=42\n    )\n    X_val, X_test, y_val, y_test, w_val, w_test = train_test_split(\n        X_temp, y_temp, w_temp, test_size=0.5, stratify=y_temp, random_state=42\n    )\n\n    results = {}\n\n    # Builders\n    def logistic_builder():\n        fraud_count = int(y_train.sum())\n        non_fraud_count = len(y_train) - fraud_count\n        cw = {0: 1.0, 1: float(non_fraud_count) / float(max(1, fraud_count))} if fraud_count > 0 else None\n        return LogisticRegression(max_iter=1000, class_weight=cw, random_state=42, solver='lbfgs')\n\n    def lgbm_builder():\n        return lgb.LGBMClassifier(n_estimators=500, random_state=42)\n\n    def xgb_builder():\n        return xgb.XGBClassifier(\n            n_estimators=500, use_label_encoder=False, eval_metric='logloss', random_state=42\n        )\n\n    builders = {\n        'logistic': (logistic_builder, True),\n        'lgbm': (lgbm_builder, False),\n        'xgb': (xgb_builder, False)\n    }\n\n    for model_key in models_to_run:\n        builder, scaler_needed = builders[model_key]\n        model = builder()\n\n        # Scaling if needed\n        if scaler_needed:\n            scaler = StandardScaler()\n            X_train_scaled = scaler.fit_transform(X_train)\n            X_val_scaled = scaler.transform(X_val)\n            X_test_scaled = scaler.transform(X_test)\n        else:\n            scaler = None\n            X_train_scaled, X_val_scaled, X_test_scaled = X_train.values, X_val.values, X_test.values\n\n        # Fit\n        model.fit(X_train_scaled, y_train, sample_weight=w_train)\n\n        # Predict probabilities\n        val_proba = model.predict_proba(X_val_scaled)[:, 1]\n        test_proba = model.predict_proba(X_test_scaled)[:, 1]\n\n        # Metrics function (weighted averages)\n        def compute_metrics(y_true, y_pred_proba, w=None):\n            y_pred = (y_pred_proba > 0.5).astype(int)\n            return {\n                'auc': roc_auc_score(y_true, y_pred_proba, sample_weight=w),\n                'accuracy': accuracy_score(y_true, y_pred, sample_weight=w),\n                'precision': precision_score(y_true, y_pred, average='weighted', sample_weight=w, zero_division=0),\n                'recall': recall_score(y_true, y_pred, average='weighted', sample_weight=w, zero_division=0),\n                'f1': f1_score(y_true, y_pred, average='weighted', sample_weight=w, zero_division=0),\n                'ks': ks_statistic(y_true, y_pred_proba)\n            }\n\n        val_metrics = compute_metrics(y_val, val_proba, w_val)\n        test_metrics = compute_metrics(y_test, test_proba, w_test)\n\n        # -----------------------------------\n        # Save Artifact to Stage\n        # -----------------------------------\n        artifact_filename = f\"{model_key}_model_{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.joblib\"\n        with tempfile.TemporaryDirectory() as tmpdir:\n            local_path = os.path.join(tmpdir, artifact_filename)\n            joblib.dump({'model': model, 'scaler': scaler, 'feature_columns': X.columns.tolist()}, local_path)\n            stage = f\"{db_for_registry}.{models_stage_name}\"\n            session.file.put(local_path, f\"@{stage}\", overwrite=True, auto_compress=False)\n        artifact_uri = f\"@{stage}/{artifact_filename}\"\n\n        # -----------------------------------\n        # CREATE TABLES IF NOT EXIST\n        # -----------------------------------\n        for table in [metrics_val_table_name, metrics_test_table_name]:\n            session.sql(f\"\"\"\n                CREATE TABLE IF NOT EXISTS {db_for_registry}.{table} (\n                    MODEL_NAME STRING,\n                    MODEL_VERSION STRING,\n                    FRAMEWORK STRING,\n                    TRAINING_DATE TIMESTAMP_NTZ,\n                    AUC FLOAT,\n                    ACCURACY FLOAT,\n                    PRECISION FLOAT,\n                    RECALL FLOAT,\n                    F1_SCORE FLOAT,\n                    KS FLOAT,\n                    CREATED_BY STRING\n                )\n            \"\"\").collect()\n\n        # -----------------------------------\n        # INSERT INTO METRICS TABLES\n        # -----------------------------------\n        version = datetime.utcnow().strftime('%Y.%m.%d.%H%M%S')\n\n        insert_sql = f\"\"\"\n        INSERT INTO {db_for_registry}.{metrics_val_table_name}\n        VALUES (?, ?, ?, CURRENT_TIMESTAMP(), ?, ?, ?, ?, ?, ?, ?)\n        \"\"\"\n        session.sql(insert_sql, params=[\n            f\"vehicle_insurance_fraud_detector_{model_key}\",\n            version, model_key,\n            val_metrics['auc'], val_metrics['accuracy'], val_metrics['precision'],\n            val_metrics['recall'], val_metrics['f1'], val_metrics['ks'], \"Luis Vejarano\"\n         ]).collect()\n\n        insert_sql2 = f\"\"\"\n        INSERT INTO {db_for_registry}.{metrics_test_table_name}\n        VALUES (?, ?, ?, CURRENT_TIMESTAMP(), ?, ?, ?, ?, ?, ?, ?)\n        \"\"\"\n        session.sql(insert_sql2, params=[\n            f\"vehicle_insurance_fraud_detector_{model_key}\",\n            version, model_key,\n            test_metrics['auc'], test_metrics['accuracy'], test_metrics['precision'],\n            test_metrics['recall'], test_metrics['f1'], test_metrics['ks'], \"Manideep\"\n        ]).collect()\n\n        # -----------------------------------\n        # REGISTER MODEL\n        # -----------------------------------\n        registry_table = f\"{db_for_registry}.{registry_table_name}\"\n        session.sql(f\"\"\"\n        CREATE TABLE IF NOT EXISTS {registry_table} (\n          MODEL_NAME STRING,\n          MODEL_VER  STRING,\n          STAGE_NAME STRING,\n          ARTIFACT_URI STRING,\n          FRAMEWORK STRING,\n          METRICS VARIANT,\n          CREATED_AT TIMESTAMP_NTZ,\n          CREATED_BY STRING\n        )\n        \"\"\").collect()\n\n        metrics_json = {\n            'validation': val_metrics,\n            'test': test_metrics\n        }\n\n        session.sql(f\"\"\"\n        INSERT INTO {registry_table}\n        (MODEL_NAME, MODEL_VER, STAGE_NAME, ARTIFACT_URI, FRAMEWORK, METRICS, CREATED_AT, CREATED_BY)\n        SELECT ?, ?, ?, ?, ?, PARSE_JSON(?), CURRENT_TIMESTAMP(), ?\n        \"\"\", params=[\n            f\"vehicle_insurance_fraud_detector_{model_key}\",\n            version, 'DEV', artifact_uri, model_key,\n            json.dumps(metrics_json), \"Manideep\"\n        ]).collect()\n\n        results[model_key] = {\n            'val_metrics': val_metrics,\n            'test_metrics': test_metrics,\n            'artifact_uri': artifact_uri\n        }\n\n        logger.info(f\"✅ Completed training & registration for {model_key}\")\n\n    return results",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d6a6c56-e415-46ba-848b-26e1c93bf612",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  }
 ]
}